{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34204d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import easydict\n",
    "import math\n",
    "# from .layers import *\n",
    "# from .models import *\n",
    "# from .utils import *\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from setuptools import setup\n",
    "from setuptools import find_packages\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "from pygcn.layers import GraphConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3717b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"C:/Users/chaeny/Desktop/DILab/Explainable AI/gcn/data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfa09f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06914547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e067a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "args = easydict.EasyDict({\n",
    "    \"no-cuda\": False,\n",
    "    \"fastmode\": False,\n",
    "    \"seed\": 42,\n",
    "    \"epoch\": 200,\n",
    "    \"lr\": 0.01,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"hidden\": 16,\n",
    "    \"dropout\": 0.5\n",
    "})\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85f19b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a19f239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc val: {:.4f}'.format(acc_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c5da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "739fd6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss train: 0.4641 acc train: 0.9143 loss val: 0.7248 acc val: 0.7900\n",
      "Epoch: 0002 loss train: 0.4231 acc train: 0.9357 loss val: 0.7244 acc val: 0.7900\n",
      "Epoch: 0003 loss train: 0.4060 acc train: 0.9357 loss val: 0.7234 acc val: 0.7900\n",
      "Epoch: 0004 loss train: 0.4054 acc train: 0.9571 loss val: 0.7207 acc val: 0.7900\n",
      "Epoch: 0005 loss train: 0.3750 acc train: 0.9643 loss val: 0.7171 acc val: 0.7900\n",
      "Epoch: 0006 loss train: 0.4170 acc train: 0.9357 loss val: 0.7131 acc val: 0.7933\n",
      "Epoch: 0007 loss train: 0.4571 acc train: 0.9429 loss val: 0.7098 acc val: 0.8000\n",
      "Epoch: 0008 loss train: 0.4240 acc train: 0.9500 loss val: 0.7070 acc val: 0.8033\n",
      "Epoch: 0009 loss train: 0.4328 acc train: 0.9429 loss val: 0.7047 acc val: 0.8067\n",
      "Epoch: 0010 loss train: 0.4287 acc train: 0.9357 loss val: 0.7030 acc val: 0.8233\n",
      "Epoch: 0011 loss train: 0.4207 acc train: 0.9429 loss val: 0.7022 acc val: 0.8200\n",
      "Epoch: 0012 loss train: 0.4275 acc train: 0.9500 loss val: 0.7018 acc val: 0.8200\n",
      "Epoch: 0013 loss train: 0.4537 acc train: 0.9214 loss val: 0.7033 acc val: 0.8033\n",
      "Epoch: 0014 loss train: 0.3904 acc train: 0.9357 loss val: 0.7054 acc val: 0.8000\n",
      "Epoch: 0015 loss train: 0.4521 acc train: 0.9357 loss val: 0.7063 acc val: 0.8000\n",
      "Epoch: 0016 loss train: 0.4486 acc train: 0.9143 loss val: 0.7061 acc val: 0.7967\n",
      "Epoch: 0017 loss train: 0.4060 acc train: 0.9143 loss val: 0.7064 acc val: 0.7967\n",
      "Epoch: 0018 loss train: 0.3595 acc train: 0.9429 loss val: 0.7059 acc val: 0.7967\n",
      "Epoch: 0019 loss train: 0.4212 acc train: 0.9357 loss val: 0.7050 acc val: 0.8000\n",
      "Epoch: 0020 loss train: 0.4270 acc train: 0.9143 loss val: 0.7030 acc val: 0.8033\n",
      "Epoch: 0021 loss train: 0.3876 acc train: 0.9500 loss val: 0.7015 acc val: 0.8033\n",
      "Epoch: 0022 loss train: 0.4051 acc train: 0.9214 loss val: 0.6996 acc val: 0.8033\n",
      "Epoch: 0023 loss train: 0.3939 acc train: 0.9286 loss val: 0.6981 acc val: 0.8033\n",
      "Epoch: 0024 loss train: 0.3918 acc train: 0.9357 loss val: 0.6973 acc val: 0.8067\n",
      "Epoch: 0025 loss train: 0.4200 acc train: 0.9143 loss val: 0.6958 acc val: 0.8033\n",
      "Epoch: 0026 loss train: 0.3894 acc train: 0.9143 loss val: 0.6963 acc val: 0.8033\n",
      "Epoch: 0027 loss train: 0.4226 acc train: 0.9571 loss val: 0.6967 acc val: 0.8033\n",
      "Epoch: 0028 loss train: 0.4468 acc train: 0.9143 loss val: 0.6974 acc val: 0.7967\n",
      "Epoch: 0029 loss train: 0.4028 acc train: 0.9357 loss val: 0.6979 acc val: 0.7967\n",
      "Epoch: 0030 loss train: 0.4156 acc train: 0.9286 loss val: 0.6977 acc val: 0.8000\n",
      "Epoch: 0031 loss train: 0.4043 acc train: 0.9214 loss val: 0.6975 acc val: 0.7967\n",
      "Epoch: 0032 loss train: 0.3987 acc train: 0.9000 loss val: 0.6972 acc val: 0.8000\n",
      "Epoch: 0033 loss train: 0.4065 acc train: 0.9429 loss val: 0.6965 acc val: 0.8033\n",
      "Epoch: 0034 loss train: 0.3981 acc train: 0.9429 loss val: 0.6945 acc val: 0.8033\n",
      "Epoch: 0035 loss train: 0.4245 acc train: 0.9071 loss val: 0.6928 acc val: 0.8067\n",
      "Epoch: 0036 loss train: 0.3971 acc train: 0.9357 loss val: 0.6907 acc val: 0.8067\n",
      "Epoch: 0037 loss train: 0.3682 acc train: 0.9643 loss val: 0.6881 acc val: 0.8067\n",
      "Epoch: 0038 loss train: 0.3697 acc train: 0.9429 loss val: 0.6873 acc val: 0.8067\n",
      "Epoch: 0039 loss train: 0.3819 acc train: 0.9429 loss val: 0.6865 acc val: 0.8100\n",
      "Epoch: 0040 loss train: 0.3523 acc train: 0.9429 loss val: 0.6869 acc val: 0.8033\n",
      "Epoch: 0041 loss train: 0.3560 acc train: 0.9500 loss val: 0.6881 acc val: 0.8033\n",
      "Epoch: 0042 loss train: 0.3710 acc train: 0.9286 loss val: 0.6895 acc val: 0.8033\n",
      "Epoch: 0043 loss train: 0.3843 acc train: 0.9714 loss val: 0.6904 acc val: 0.8033\n",
      "Epoch: 0044 loss train: 0.3559 acc train: 0.9500 loss val: 0.6908 acc val: 0.8033\n",
      "Epoch: 0045 loss train: 0.3868 acc train: 0.9143 loss val: 0.6914 acc val: 0.8067\n",
      "Epoch: 0046 loss train: 0.3523 acc train: 0.9500 loss val: 0.6918 acc val: 0.8067\n",
      "Epoch: 0047 loss train: 0.3612 acc train: 0.9500 loss val: 0.6897 acc val: 0.8067\n",
      "Epoch: 0048 loss train: 0.3715 acc train: 0.9214 loss val: 0.6858 acc val: 0.8033\n",
      "Epoch: 0049 loss train: 0.3985 acc train: 0.9500 loss val: 0.6828 acc val: 0.8067\n",
      "Epoch: 0050 loss train: 0.3554 acc train: 0.9429 loss val: 0.6804 acc val: 0.8067\n",
      "Epoch: 0051 loss train: 0.3594 acc train: 0.9500 loss val: 0.6783 acc val: 0.8100\n",
      "Epoch: 0052 loss train: 0.3622 acc train: 0.9571 loss val: 0.6759 acc val: 0.8133\n",
      "Epoch: 0053 loss train: 0.3709 acc train: 0.9429 loss val: 0.6748 acc val: 0.8100\n",
      "Epoch: 0054 loss train: 0.3713 acc train: 0.9571 loss val: 0.6752 acc val: 0.8067\n",
      "Epoch: 0055 loss train: 0.3767 acc train: 0.9500 loss val: 0.6768 acc val: 0.8067\n",
      "Epoch: 0056 loss train: 0.3625 acc train: 0.9286 loss val: 0.6785 acc val: 0.8067\n",
      "Epoch: 0057 loss train: 0.4046 acc train: 0.9286 loss val: 0.6793 acc val: 0.8033\n",
      "Epoch: 0058 loss train: 0.3815 acc train: 0.9214 loss val: 0.6803 acc val: 0.8033\n",
      "Epoch: 0059 loss train: 0.3685 acc train: 0.9357 loss val: 0.6814 acc val: 0.8033\n",
      "Epoch: 0060 loss train: 0.3649 acc train: 0.9429 loss val: 0.6822 acc val: 0.8033\n",
      "Epoch: 0061 loss train: 0.3594 acc train: 0.9571 loss val: 0.6815 acc val: 0.8033\n",
      "Epoch: 0062 loss train: 0.3595 acc train: 0.9500 loss val: 0.6801 acc val: 0.8033\n",
      "Epoch: 0063 loss train: 0.3577 acc train: 0.9643 loss val: 0.6766 acc val: 0.8033\n",
      "Epoch: 0064 loss train: 0.3461 acc train: 0.9429 loss val: 0.6745 acc val: 0.8100\n",
      "Epoch: 0065 loss train: 0.3284 acc train: 0.9643 loss val: 0.6719 acc val: 0.8100\n",
      "Epoch: 0066 loss train: 0.3622 acc train: 0.9643 loss val: 0.6699 acc val: 0.8100\n",
      "Epoch: 0067 loss train: 0.3659 acc train: 0.9429 loss val: 0.6684 acc val: 0.8100\n",
      "Epoch: 0068 loss train: 0.3679 acc train: 0.9571 loss val: 0.6677 acc val: 0.8100\n",
      "Epoch: 0069 loss train: 0.3699 acc train: 0.9357 loss val: 0.6667 acc val: 0.8100\n",
      "Epoch: 0070 loss train: 0.3082 acc train: 0.9500 loss val: 0.6663 acc val: 0.8100\n",
      "Epoch: 0071 loss train: 0.3467 acc train: 0.9571 loss val: 0.6676 acc val: 0.8100\n",
      "Epoch: 0072 loss train: 0.3452 acc train: 0.9643 loss val: 0.6698 acc val: 0.8100\n",
      "Epoch: 0073 loss train: 0.3235 acc train: 0.9500 loss val: 0.6720 acc val: 0.8133\n",
      "Epoch: 0074 loss train: 0.3584 acc train: 0.9286 loss val: 0.6725 acc val: 0.8100\n",
      "Epoch: 0075 loss train: 0.3529 acc train: 0.9357 loss val: 0.6718 acc val: 0.8100\n",
      "Epoch: 0076 loss train: 0.3650 acc train: 0.9429 loss val: 0.6714 acc val: 0.8100\n",
      "Epoch: 0077 loss train: 0.3607 acc train: 0.9571 loss val: 0.6704 acc val: 0.8100\n",
      "Epoch: 0078 loss train: 0.3480 acc train: 0.9643 loss val: 0.6690 acc val: 0.8033\n",
      "Epoch: 0079 loss train: 0.3601 acc train: 0.9571 loss val: 0.6684 acc val: 0.8033\n",
      "Epoch: 0080 loss train: 0.3994 acc train: 0.9214 loss val: 0.6673 acc val: 0.8067\n",
      "Epoch: 0081 loss train: 0.3516 acc train: 0.9214 loss val: 0.6668 acc val: 0.8067\n",
      "Epoch: 0082 loss train: 0.3418 acc train: 0.9357 loss val: 0.6668 acc val: 0.8067\n",
      "Epoch: 0083 loss train: 0.3283 acc train: 0.9643 loss val: 0.6674 acc val: 0.8067\n",
      "Epoch: 0084 loss train: 0.3668 acc train: 0.9357 loss val: 0.6682 acc val: 0.8067\n",
      "Epoch: 0085 loss train: 0.3247 acc train: 0.9786 loss val: 0.6690 acc val: 0.8100\n",
      "Epoch: 0086 loss train: 0.3637 acc train: 0.9357 loss val: 0.6698 acc val: 0.8067\n",
      "Epoch: 0087 loss train: 0.3418 acc train: 0.9500 loss val: 0.6712 acc val: 0.8100\n",
      "Epoch: 0088 loss train: 0.3328 acc train: 0.9571 loss val: 0.6702 acc val: 0.8100\n",
      "Epoch: 0089 loss train: 0.3150 acc train: 0.9571 loss val: 0.6696 acc val: 0.8133\n",
      "Epoch: 0090 loss train: 0.3448 acc train: 0.9357 loss val: 0.6672 acc val: 0.8067\n",
      "Epoch: 0091 loss train: 0.3394 acc train: 0.9429 loss val: 0.6659 acc val: 0.8067\n",
      "Epoch: 0092 loss train: 0.3489 acc train: 0.9357 loss val: 0.6665 acc val: 0.8033\n",
      "Epoch: 0093 loss train: 0.3377 acc train: 0.9429 loss val: 0.6667 acc val: 0.8033\n",
      "Epoch: 0094 loss train: 0.3061 acc train: 0.9643 loss val: 0.6670 acc val: 0.8033\n",
      "Epoch: 0095 loss train: 0.3324 acc train: 0.9714 loss val: 0.6659 acc val: 0.8067\n",
      "Epoch: 0096 loss train: 0.3486 acc train: 0.9357 loss val: 0.6656 acc val: 0.8033\n",
      "Epoch: 0097 loss train: 0.3367 acc train: 0.9429 loss val: 0.6653 acc val: 0.8067\n",
      "Epoch: 0098 loss train: 0.3980 acc train: 0.9214 loss val: 0.6645 acc val: 0.8067\n",
      "Epoch: 0099 loss train: 0.3446 acc train: 0.9429 loss val: 0.6641 acc val: 0.8033\n",
      "Epoch: 0100 loss train: 0.3626 acc train: 0.9286 loss val: 0.6647 acc val: 0.8100\n",
      "Epoch: 0101 loss train: 0.3305 acc train: 0.9643 loss val: 0.6645 acc val: 0.8033\n",
      "Epoch: 0102 loss train: 0.3342 acc train: 0.9643 loss val: 0.6635 acc val: 0.8100\n",
      "Epoch: 0103 loss train: 0.3505 acc train: 0.9500 loss val: 0.6607 acc val: 0.8067\n",
      "Epoch: 0104 loss train: 0.3881 acc train: 0.9357 loss val: 0.6574 acc val: 0.8067\n",
      "Epoch: 0105 loss train: 0.3282 acc train: 0.9429 loss val: 0.6564 acc val: 0.8100\n",
      "Epoch: 0106 loss train: 0.3556 acc train: 0.9357 loss val: 0.6567 acc val: 0.8067\n",
      "Epoch: 0107 loss train: 0.3324 acc train: 0.9429 loss val: 0.6565 acc val: 0.8033\n",
      "Epoch: 0108 loss train: 0.3124 acc train: 0.9786 loss val: 0.6560 acc val: 0.8033\n",
      "Epoch: 0109 loss train: 0.2722 acc train: 0.9714 loss val: 0.6550 acc val: 0.8033\n",
      "Epoch: 0110 loss train: 0.3728 acc train: 0.9071 loss val: 0.6549 acc val: 0.8067\n",
      "Epoch: 0111 loss train: 0.3662 acc train: 0.9357 loss val: 0.6556 acc val: 0.8100\n",
      "Epoch: 0112 loss train: 0.3562 acc train: 0.9357 loss val: 0.6570 acc val: 0.8033\n",
      "Epoch: 0113 loss train: 0.3499 acc train: 0.9500 loss val: 0.6579 acc val: 0.8067\n",
      "Epoch: 0114 loss train: 0.3104 acc train: 0.9571 loss val: 0.6603 acc val: 0.8133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0115 loss train: 0.3166 acc train: 0.9429 loss val: 0.6617 acc val: 0.8133\n",
      "Epoch: 0116 loss train: 0.3408 acc train: 0.9429 loss val: 0.6605 acc val: 0.8100\n",
      "Epoch: 0117 loss train: 0.3055 acc train: 0.9643 loss val: 0.6587 acc val: 0.8067\n",
      "Epoch: 0118 loss train: 0.2965 acc train: 0.9714 loss val: 0.6565 acc val: 0.8000\n",
      "Epoch: 0119 loss train: 0.3115 acc train: 0.9571 loss val: 0.6545 acc val: 0.8100\n",
      "Epoch: 0120 loss train: 0.3330 acc train: 0.9429 loss val: 0.6533 acc val: 0.8100\n",
      "Epoch: 0121 loss train: 0.3069 acc train: 0.9357 loss val: 0.6521 acc val: 0.8067\n",
      "Epoch: 0122 loss train: 0.3220 acc train: 0.9357 loss val: 0.6523 acc val: 0.8067\n",
      "Epoch: 0123 loss train: 0.2925 acc train: 0.9571 loss val: 0.6529 acc val: 0.8067\n",
      "Epoch: 0124 loss train: 0.3442 acc train: 0.9286 loss val: 0.6537 acc val: 0.8133\n",
      "Epoch: 0125 loss train: 0.3598 acc train: 0.9571 loss val: 0.6539 acc val: 0.8133\n",
      "Epoch: 0126 loss train: 0.3178 acc train: 0.9786 loss val: 0.6538 acc val: 0.8067\n",
      "Epoch: 0127 loss train: 0.3266 acc train: 0.9571 loss val: 0.6539 acc val: 0.8067\n",
      "Epoch: 0128 loss train: 0.3436 acc train: 0.9214 loss val: 0.6539 acc val: 0.8067\n",
      "Epoch: 0129 loss train: 0.2994 acc train: 0.9500 loss val: 0.6541 acc val: 0.8033\n",
      "Epoch: 0130 loss train: 0.3186 acc train: 0.9357 loss val: 0.6546 acc val: 0.8033\n",
      "Epoch: 0131 loss train: 0.2772 acc train: 0.9857 loss val: 0.6552 acc val: 0.8067\n",
      "Epoch: 0132 loss train: 0.3079 acc train: 0.9643 loss val: 0.6536 acc val: 0.8100\n",
      "Epoch: 0133 loss train: 0.3026 acc train: 0.9643 loss val: 0.6534 acc val: 0.8100\n",
      "Epoch: 0134 loss train: 0.3073 acc train: 0.9500 loss val: 0.6536 acc val: 0.8100\n",
      "Epoch: 0135 loss train: 0.3225 acc train: 0.9571 loss val: 0.6537 acc val: 0.8067\n",
      "Epoch: 0136 loss train: 0.3461 acc train: 0.9286 loss val: 0.6542 acc val: 0.8067\n",
      "Epoch: 0137 loss train: 0.3096 acc train: 0.9571 loss val: 0.6554 acc val: 0.8033\n",
      "Epoch: 0138 loss train: 0.3667 acc train: 0.9286 loss val: 0.6576 acc val: 0.8033\n",
      "Epoch: 0139 loss train: 0.3064 acc train: 0.9500 loss val: 0.6590 acc val: 0.8033\n",
      "Epoch: 0140 loss train: 0.3200 acc train: 0.9571 loss val: 0.6594 acc val: 0.8100\n",
      "Epoch: 0141 loss train: 0.3648 acc train: 0.9643 loss val: 0.6579 acc val: 0.8133\n",
      "Epoch: 0142 loss train: 0.3225 acc train: 0.9714 loss val: 0.6569 acc val: 0.8133\n",
      "Epoch: 0143 loss train: 0.2765 acc train: 0.9643 loss val: 0.6552 acc val: 0.8100\n",
      "Epoch: 0144 loss train: 0.3140 acc train: 0.9429 loss val: 0.6527 acc val: 0.8033\n",
      "Epoch: 0145 loss train: 0.3068 acc train: 0.9500 loss val: 0.6500 acc val: 0.8000\n",
      "Epoch: 0146 loss train: 0.3251 acc train: 0.9571 loss val: 0.6482 acc val: 0.8100\n",
      "Epoch: 0147 loss train: 0.3250 acc train: 0.9643 loss val: 0.6478 acc val: 0.8067\n",
      "Epoch: 0148 loss train: 0.2709 acc train: 0.9643 loss val: 0.6471 acc val: 0.8067\n",
      "Epoch: 0149 loss train: 0.3365 acc train: 0.9429 loss val: 0.6476 acc val: 0.8067\n",
      "Epoch: 0150 loss train: 0.3256 acc train: 0.9571 loss val: 0.6493 acc val: 0.8033\n",
      "Epoch: 0151 loss train: 0.2838 acc train: 0.9571 loss val: 0.6512 acc val: 0.8033\n",
      "Epoch: 0152 loss train: 0.2982 acc train: 0.9429 loss val: 0.6523 acc val: 0.8000\n",
      "Epoch: 0153 loss train: 0.3268 acc train: 0.9571 loss val: 0.6546 acc val: 0.8067\n",
      "Epoch: 0154 loss train: 0.2915 acc train: 0.9714 loss val: 0.6554 acc val: 0.8067\n",
      "Epoch: 0155 loss train: 0.2737 acc train: 0.9786 loss val: 0.6537 acc val: 0.8033\n",
      "Epoch: 0156 loss train: 0.2829 acc train: 0.9643 loss val: 0.6517 acc val: 0.8067\n",
      "Epoch: 0157 loss train: 0.3051 acc train: 0.9429 loss val: 0.6500 acc val: 0.8133\n",
      "Epoch: 0158 loss train: 0.3076 acc train: 0.9571 loss val: 0.6492 acc val: 0.8067\n",
      "Epoch: 0159 loss train: 0.3251 acc train: 0.9500 loss val: 0.6489 acc val: 0.8033\n",
      "Epoch: 0160 loss train: 0.3248 acc train: 0.9714 loss val: 0.6483 acc val: 0.8067\n",
      "Epoch: 0161 loss train: 0.3182 acc train: 0.9571 loss val: 0.6480 acc val: 0.8100\n",
      "Epoch: 0162 loss train: 0.3328 acc train: 0.9357 loss val: 0.6467 acc val: 0.8100\n",
      "Epoch: 0163 loss train: 0.3136 acc train: 0.9429 loss val: 0.6452 acc val: 0.8100\n",
      "Epoch: 0164 loss train: 0.3171 acc train: 0.9429 loss val: 0.6433 acc val: 0.8067\n",
      "Epoch: 0165 loss train: 0.3253 acc train: 0.9786 loss val: 0.6411 acc val: 0.8067\n",
      "Epoch: 0166 loss train: 0.3054 acc train: 0.9643 loss val: 0.6394 acc val: 0.8033\n",
      "Epoch: 0167 loss train: 0.2602 acc train: 0.9643 loss val: 0.6385 acc val: 0.8033\n",
      "Epoch: 0168 loss train: 0.2674 acc train: 0.9429 loss val: 0.6385 acc val: 0.8033\n",
      "Epoch: 0169 loss train: 0.3265 acc train: 0.9643 loss val: 0.6399 acc val: 0.8033\n",
      "Epoch: 0170 loss train: 0.2900 acc train: 0.9571 loss val: 0.6428 acc val: 0.8000\n",
      "Epoch: 0171 loss train: 0.3172 acc train: 0.9643 loss val: 0.6461 acc val: 0.8067\n",
      "Epoch: 0172 loss train: 0.3416 acc train: 0.9429 loss val: 0.6494 acc val: 0.8067\n",
      "Epoch: 0173 loss train: 0.2830 acc train: 0.9714 loss val: 0.6538 acc val: 0.8033\n",
      "Epoch: 0174 loss train: 0.3574 acc train: 0.9500 loss val: 0.6557 acc val: 0.8067\n",
      "Epoch: 0175 loss train: 0.2968 acc train: 0.9357 loss val: 0.6543 acc val: 0.8067\n",
      "Epoch: 0176 loss train: 0.2937 acc train: 0.9500 loss val: 0.6523 acc val: 0.8067\n",
      "Epoch: 0177 loss train: 0.2872 acc train: 0.9786 loss val: 0.6491 acc val: 0.8100\n",
      "Epoch: 0178 loss train: 0.3339 acc train: 0.9357 loss val: 0.6464 acc val: 0.8067\n",
      "Epoch: 0179 loss train: 0.3078 acc train: 0.9357 loss val: 0.6452 acc val: 0.8033\n",
      "Epoch: 0180 loss train: 0.2881 acc train: 0.9500 loss val: 0.6447 acc val: 0.8033\n",
      "Epoch: 0181 loss train: 0.2819 acc train: 0.9643 loss val: 0.6455 acc val: 0.8033\n",
      "Epoch: 0182 loss train: 0.3109 acc train: 0.9643 loss val: 0.6465 acc val: 0.8100\n",
      "Epoch: 0183 loss train: 0.2688 acc train: 0.9714 loss val: 0.6473 acc val: 0.8100\n",
      "Epoch: 0184 loss train: 0.2764 acc train: 0.9643 loss val: 0.6501 acc val: 0.8100\n",
      "Epoch: 0185 loss train: 0.3240 acc train: 0.9357 loss val: 0.6523 acc val: 0.8067\n",
      "Epoch: 0186 loss train: 0.2939 acc train: 0.9857 loss val: 0.6543 acc val: 0.8100\n",
      "Epoch: 0187 loss train: 0.2628 acc train: 0.9857 loss val: 0.6556 acc val: 0.8100\n",
      "Epoch: 0188 loss train: 0.3074 acc train: 0.9500 loss val: 0.6567 acc val: 0.8067\n",
      "Epoch: 0189 loss train: 0.3130 acc train: 0.9500 loss val: 0.6544 acc val: 0.8033\n",
      "Epoch: 0190 loss train: 0.2511 acc train: 0.9786 loss val: 0.6516 acc val: 0.8033\n",
      "Epoch: 0191 loss train: 0.2827 acc train: 0.9500 loss val: 0.6503 acc val: 0.8100\n",
      "Epoch: 0192 loss train: 0.2917 acc train: 0.9500 loss val: 0.6485 acc val: 0.8033\n",
      "Epoch: 0193 loss train: 0.2705 acc train: 0.9643 loss val: 0.6475 acc val: 0.8000\n",
      "Epoch: 0194 loss train: 0.3099 acc train: 0.9571 loss val: 0.6473 acc val: 0.8000\n",
      "Epoch: 0195 loss train: 0.2537 acc train: 0.9714 loss val: 0.6468 acc val: 0.8000\n",
      "Epoch: 0196 loss train: 0.3217 acc train: 0.9571 loss val: 0.6454 acc val: 0.8000\n",
      "Epoch: 0197 loss train: 0.3145 acc train: 0.9429 loss val: 0.6426 acc val: 0.7967\n",
      "Epoch: 0198 loss train: 0.3132 acc train: 0.9571 loss val: 0.6422 acc val: 0.8067\n",
      "Epoch: 0199 loss train: 0.2874 acc train: 0.9643 loss val: 0.6438 acc val: 0.8067\n",
      "Epoch: 0200 loss train: 0.2984 acc train: 0.9571 loss val: 0.6446 acc val: 0.8067\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.3384s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epoch):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce6c89a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6441 accuracy= 0.8260\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114238c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
